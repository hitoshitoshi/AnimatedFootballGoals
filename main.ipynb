{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"TestGoal.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import Video\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "testFile = \"TestGoal.mp4\"\n",
    "target_fps = 10\n",
    "\n",
    "Video(\"TestGoal.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to 10 fps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"fpsAdjusted.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(testFile)\n",
    "\n",
    "# Get the original FPS and dimensions\n",
    "original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Calculate how many frames we skip between each frame we keep\n",
    "# (rounding to ensure we don't miss frames due to float precision)\n",
    "frame_interval = int(round(original_fps / target_fps))\n",
    "\n",
    "# Define output codec and create VideoWriter\n",
    "fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "out = cv2.VideoWriter('fpsAdjusted.mp4', fourcc, target_fps, (width, height))\n",
    "\n",
    "frame_index = 0\n",
    "written_frames = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Write only every \"frame_interval\"-th frame\n",
    "    if frame_index % frame_interval == 0:\n",
    "        out.write(frame)\n",
    "        written_frames += 1\n",
    "    \n",
    "    frame_index += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "Video(\"fpsAdjusted.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the Ball:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ball detected at frame 3 with confidence 0.61\n",
      "Initial center: (2793, 1487) Initial radius: 18\n"
     ]
    }
   ],
   "source": [
    "# Load YOLOv8 model.\n",
    "model = YOLO(\"yolov8x.pt\")  # Use the nano model (or choose another)\n",
    "\n",
    "# Open your video.\n",
    "video_path = \"fpsAdjusted.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video:\", video_path)\n",
    "    exit()\n",
    "\n",
    "# Read the first frame to get dimensions.\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    cap.release()\n",
    "    exit(\"Error reading video frame.\")\n",
    "\n",
    "# Define width, height, and (optionally) target fps.\n",
    "height, width = frame.shape[:2]\n",
    "target_fps = cap.get(cv2.CAP_PROP_FPS)  # you can adjust if you like\n",
    "\n",
    "# Parameters for detection.\n",
    "detection_threshold = 0.5  # Only accept detections with confidence > 0.8\n",
    "ball_class = \"sports ball\"  # Label expected in YOLO. (COCO uses \"sports ball\")\n",
    "center = None\n",
    "circle_radius = None\n",
    "frame_idx = 0\n",
    "\n",
    "# Loop through frames until we get a high-confidence detection.\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # Reached end of video without detection\n",
    "\n",
    "    # Run YOLO inference on the frame.\n",
    "    # (Set a lower conf here to not miss detections; we filter after.)\n",
    "    results = model(frame, conf=0.5, verbose=False)\n",
    "    \n",
    "    # Iterate through detections.\n",
    "    for box in results[0].boxes:\n",
    "        cls_id = int(box.cls[0].item())\n",
    "        conf = box.conf[0].item()\n",
    "        label = model.names[cls_id] if hasattr(model, \"names\") else str(cls_id)\n",
    "        \n",
    "        # Check for the ball with high confidence.\n",
    "        if label == ball_class and conf >= detection_threshold:\n",
    "            # Extract bounding box coordinates (xyxy format).\n",
    "            x_min, y_min, x_max, y_max = box.xyxy[0].tolist()\n",
    "            # Compute the center as the midpoint of the bounding box.\n",
    "            center_x = (x_min + x_max) / 2\n",
    "            center_y = (y_min + y_max) / 2\n",
    "            center = (int(center_x), int(center_y))\n",
    "            # Estimate the ball's radius (average half-width and half-height).\n",
    "            width_box = x_max - x_min\n",
    "            height_box = y_max - y_min\n",
    "            circle_radius = int((width_box + height_box) / 4)\n",
    "            print(f\"Ball detected at frame {frame_idx} with confidence {conf:.2f}\")\n",
    "            print(\"Initial center:\", center, \"Initial radius:\", circle_radius)\n",
    "            break\n",
    "    if center is not None:\n",
    "        break\n",
    "    frame_idx += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ball Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"ball_tracked.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windowWidth, windowHeight = width // 10, height // 10\n",
    "threshold = 25.0               # Color similarity threshold (Euclidean distance)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "out = cv2.VideoWriter(\"ball_tracked.mp4\", fourcc, target_fps, (width, height))\n",
    "\n",
    "# Create an initial mask and compute the initial average color.\n",
    "mask_init = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
    "cv2.circle(mask_init, center, circle_radius, 255, thickness=-1)\n",
    "avg_color = cv2.mean(frame, mask=mask_init)[:3]  # (B, G, R)\n",
    "avg_color = np.array(avg_color, dtype=float)\n",
    "\n",
    "# --- Process Each Frame ---\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Define the window boundaries (ensure they remain within image limits).\n",
    "    start_x = max(center[0] - windowWidth, 0)\n",
    "    end_x   = min(center[0] + windowWidth, width)\n",
    "    start_y = max(center[1] - windowHeight, 0)\n",
    "    end_y   = min(center[1] + windowHeight, height)\n",
    "\n",
    "    similar_coords = []  # List for storing coordinates of pixels that meet the threshold.\n",
    "    for y in range(start_y, end_y):\n",
    "        for x in range(start_x, end_x):\n",
    "            # Compute Euclidean distance in RGB space using the original frame colors.\n",
    "            diff = np.linalg.norm(frame[y, x].astype(float) - avg_color)\n",
    "            if diff < threshold:\n",
    "                similar_coords.append((x, y))\n",
    "                \n",
    "    # Update the average color and the center based on the similar pixels.\n",
    "    if similar_coords:\n",
    "        # Build a proper mask for the similar pixels.\n",
    "        mask_similar = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
    "        for (x, y) in similar_coords:\n",
    "            mask_similar[y, x] = 255\n",
    "\n",
    "        # Compute the new average based on the pixels in this mask.\n",
    "        new_avg = np.array(cv2.mean(frame, mask=mask_similar)[:3], dtype=float)\n",
    "\n",
    "        # Calculate learning rate based on the number of similar pixels relative to window area.\n",
    "        n = len(similar_coords)\n",
    "        window_area = (end_x - start_x) * (end_y - start_y)\n",
    "        ratio = (float(n) / window_area)\n",
    "        lr = ratio * ratio * ratio\n",
    "\n",
    "        # Update the average color with the weighted combination.\n",
    "        avg_color = (1 - lr) * avg_color + lr * new_avg\n",
    "        # Compute the centroid of the similar pixels.\n",
    "        similar_coords = np.array(similar_coords)\n",
    "        centroid_x = int(np.mean(similar_coords[:, 0]))\n",
    "        centroid_y = int(np.mean(similar_coords[:, 1]))\n",
    "        center = (centroid_x, centroid_y)\n",
    "\n",
    "        # Draw the final red circle (with radius 20) at the computed centroid.\n",
    "        cv2.circle(frame, center, circle_radius, (0, 0, 255), thickness=2)\n",
    "    # Write the processed frame to the output video.\n",
    "    out.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "Video(\"ball_tracked.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pitch Line tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"pitch_lines.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Open the video\n",
    "video_path = \"ball_tracked.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video:\", video_path)\n",
    "    exit()\n",
    "\n",
    "# Read the first frame to get dimensions.\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    cap.release()\n",
    "    exit(\"Error reading video frame.\")\n",
    "\n",
    "height, width = frame.shape[:2]\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# ---- Calculate scaling factor based on a 4k base resolution (3840x2160) ----\n",
    "BASE_WIDTH = 3840\n",
    "scalingFactor = width / BASE_WIDTH\n",
    "\n",
    "# ---- Scale parameters accordingly ----\n",
    "# These values were tuned for a 4k video. Now they are adjusted relative to the current resolution.\n",
    "minLineLength_scaled = int(500 * scalingFactor)\n",
    "maxLineGap_scaled    = int(100 * scalingFactor)\n",
    "hough_threshold_scaled = int(700 * scalingFactor)\n",
    "\n",
    "# For the morphological kernel, scale the kernel size.\n",
    "# We make sure the kernel size remains at least 3 and odd (if needed) for proper morphology operations.\n",
    "kernel_size = max(3, int(3 * scalingFactor))\n",
    "if kernel_size % 2 == 0:\n",
    "    kernel_size += 1\n",
    "\n",
    "# Set up VideoWriter to save the processed video.\n",
    "fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "out = cv2.VideoWriter(\"pitch_lines.mp4\", fourcc, fps, (width, height))\n",
    "\n",
    "# Reset the capture to the beginning.\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "# ---- Process Each Frame in the Video ----\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to HSV color space.\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the HSV range for white colors.\n",
    "    lower_white = np.array([20, 20, 140])\n",
    "    upper_white = np.array([100, 100, 255])\n",
    "    mask_white = cv2.inRange(hsv, lower_white, upper_white)\n",
    "\n",
    "    # Apply morphological operations to reduce noise.\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size, kernel_size))\n",
    "    mask_clean = cv2.morphologyEx(mask_white, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "    \n",
    "    # Apply Canny edge detection on the cleaned mask.\n",
    "    edges = cv2.Canny(mask_clean, 50, 100)\n",
    "\n",
    "    # Detect lines using the probabilistic Hough transform with scaled parameters.\n",
    "    lines = cv2.HoughLinesP(mask_clean,\n",
    "                            rho=1.0,\n",
    "                            theta=np.pi/360,\n",
    "                            threshold=hough_threshold_scaled,\n",
    "                            minLineLength=minLineLength_scaled,\n",
    "                            maxLineGap=maxLineGap_scaled)\n",
    "\n",
    "    # If lines are found, draw them on the frame.\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            cv2.line(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "    \n",
    "    # Write the processed frame to the output video.\n",
    "    out.write(frame)\n",
    "\n",
    "# Release the resources.\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Optionally, play the video (depending on your environment)\n",
    "Video(\"pitch_lines.mp4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal Post detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"goalpost.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Open the video\n",
    "video_path = \"fpsAdjusted.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video:\", video_path)\n",
    "    exit()\n",
    "\n",
    "# Read the first frame to get dimensions.\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    cap.release()\n",
    "    exit(\"Error reading video frame.\")\n",
    "\n",
    "height, width = frame.shape[:2]\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# ---- Calculate scaling factor based on a 4k base resolution (3840x2160) ----\n",
    "BASE_WIDTH = 3840\n",
    "scalingFactor = width / BASE_WIDTH\n",
    "\n",
    "# ---- Scale parameters accordingly ----\n",
    "rho_scaled = 1.0 * scalingFactor                  # Scale rho parameter\n",
    "hough_threshold_scaled = int(100 * scalingFactor)   # Scale Hough threshold\n",
    "minLineLength_scaled = int(500 * scalingFactor)       # Scale minimum line length\n",
    "maxLineGap_scaled = int(100 * scalingFactor)          # Scale maximum line gap\n",
    "line_thickness_scaled = max(1, int(50 * scalingFactor))  # Scale line thickness\n",
    "\n",
    "# For the morphological kernel, scale the kernel size.\n",
    "kernel_size = max(3, int(3 * scalingFactor))\n",
    "if kernel_size % 2 == 0:  # Ensure an odd kernel size if needed\n",
    "    kernel_size += 1\n",
    "\n",
    "# Set up VideoWriter to save the processed video.\n",
    "fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "out = cv2.VideoWriter(\"goalpost.mp4\", fourcc, fps, (width, height))\n",
    "\n",
    "# Reset the capture to the beginning.\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "# ---- Process Each Frame in the Video ----\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to HSV color space.\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define a broad range for white colors.\n",
    "    lower_white = np.array([0, 0, 150])\n",
    "    upper_white = np.array([200, 50, 255])\n",
    "    mask_white = cv2.inRange(hsv, lower_white, upper_white)\n",
    "\n",
    "    # Apply morphological operations to reduce noise in the mask.\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size, kernel_size))\n",
    "    mask_clean = cv2.morphologyEx(mask_white, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "    \n",
    "    # Apply Canny edge detection on the cleaned mask.\n",
    "    edges = cv2.Canny(mask_clean, 50, 100)\n",
    "\n",
    "    # Detect lines in the edge image using probabilistic Hough transform with scaled parameters.\n",
    "    lines = cv2.HoughLinesP(edges,\n",
    "                            rho=rho_scaled,\n",
    "                            theta=np.pi/180,\n",
    "                            threshold=hough_threshold_scaled,\n",
    "                            minLineLength=minLineLength_scaled,\n",
    "                            maxLineGap=maxLineGap_scaled)\n",
    "\n",
    "    # If lines are found, draw them on the frame.\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            cv2.line(frame, (x1, y1), (x2, y2), (255, 0, 0), line_thickness_scaled)\n",
    "    \n",
    "    # Write the processed frame to the output video.\n",
    "    out.write(frame)\n",
    "    # Optionally, if you want to see the edge map, you can convert and write it:\n",
    "    # edges_bgr = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n",
    "    # out.write(edges_bgr)\n",
    "\n",
    "# Release resources.\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Optionally, display the processed video in your environment:\n",
    "Video(\"goalpost.mp4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Player Detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"yolo_seg_ball_centered.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load YOLOv8 **segmentation model**\n",
    "model = YOLO(\"yolo11x-seg.pt\")  # or yolov8s-seg.pt, yolov8m-seg.pt, etc.\n",
    "\n",
    "# Open video\n",
    "video_path = \"fpsAdjusted.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    raise Exception(\"Error opening video\")\n",
    "\n",
    "# Get video properties\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter(\"yolo_seg_ball_centered.mp4\", cv2.VideoWriter_fourcc(*'avc1'), fps, (width, height))\n",
    "\n",
    "# Dummy ball center (replace with actual tracking)\n",
    "def get_ball_center(frame):\n",
    "    return (width // 2, height // 2)\n",
    "\n",
    "scalingFactor = width / BASE_WIDTH\n",
    "\n",
    "\n",
    "CROP_SIZE = 800\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Get ball center and define crop area\n",
    "    cx, cy = get_ball_center(frame)\n",
    "    x1 = max(cx - CROP_SIZE // 2, 0)\n",
    "    y1 = max(cy - CROP_SIZE // 2, 0)\n",
    "    x2 = min(cx + CROP_SIZE // 2, width)\n",
    "    y2 = min(cy + CROP_SIZE // 2, height)\n",
    "    cropped_frame = frame[y1:y2, x1:x2]\n",
    "\n",
    "    # Run instance segmentation on crop\n",
    "    results = model(cropped_frame, task='segment', conf=0.2, iou=0.4, verbose=False)[0]\n",
    "\n",
    "    # Loop over all detected masks\n",
    "    if results.masks is not None:\n",
    "        masks = results.masks.data.cpu().numpy()  # shape: [N, H, W]\n",
    "        boxes = results.boxes.xyxy.cpu().numpy()\n",
    "        classes = results.boxes.cls.cpu().numpy()\n",
    "        confs = results.boxes.conf.cpu().numpy()\n",
    "\n",
    "        for mask, box, cls, conf in zip(masks, boxes, classes, confs):\n",
    "            if int(cls) != 0:\n",
    "                continue  # skip non-persons\n",
    "\n",
    "            # Resize mask to match crop location in full frame\n",
    "            mask_resized = np.zeros((height, width), dtype=np.uint8)\n",
    "            mask_bin = cv2.resize((mask * 255).astype(np.uint8), (x2 - x1, y2 - y1), interpolation=cv2.INTER_NEAREST)\n",
    "            mask_resized[y1:y2, x1:x2] = mask_bin\n",
    "\n",
    "            # Optional: color fill the player mask\n",
    "            colored = cv2.bitwise_and(frame, frame, mask=mask_resized)\n",
    "            frame = cv2.addWeighted(frame, 1.0, colored, 0.6, 0)\n",
    "\n",
    "            # Draw green contour\n",
    "            contours, _ = cv2.findContours(mask_resized, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(frame, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "            # Draw class label\n",
    "            x1_box = int(box[0]) + x1\n",
    "            y1_box = int(box[1]) + y1\n",
    "            cv2.putText(frame, f\"Player {conf:.2f}\", (x1_box, y1_box - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    " \n",
    "    # Optional: show crop window and ball\n",
    "    cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 1)\n",
    "    cv2.circle(frame, (cx, cy), 5, (0, 0, 255), -1)\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "Video(\"yolo_seg_ball_centered.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
